---
title: "Subgrouping Self V1"
format: 
  dashboard:
    theme: darkly
---

# Design

::: {.card title="Summary"}

Participant selects political party affiliation which then becomes the political party of the majority group. 

Majority group opinions deviate away from participant’s choice on 8 issues with possible condition randomly selected (deviation threshold = [0/25/50/75/100%]). 

Hypothesis: 

- Greater deviation will lead to self-subtyping. 
- Greater homogeneity will increase participant’s confidence about new agent’s position.

Method: 8 issues, 8 agents; deviance is across agents within each issue (deviance not linked to specific agent; deviant agents randomized within each issue)

:::

::: {.card title="Instructions"}

On trial 1: "On the screens that follow you're going to learn about a collection of people we polled on a series of political issues. Like yourself, they also most identify with the Independent party. You are going to make guesses about and receive feedback on their positions on a series of 8 political issues. You don't have to remember what each person's position is, but try to see if you can figure out to what extent each person agrees with everyone else."

:::


```{r}
pacman::p_load(tidyverse,
               table1,
               ggplot2,
               plotly,
               tidymodels,
               multilevelmod,
               broom.mixed,
               lme4,
               emmeans,
               lmerTest,
               tidyjson)
emm_options(lmer.df = 'satterthwaite')
emm_options(lmerTest.limit = 20000)
#to run the dashboard, need to install development version of Quarto:
#https://quarto.org/docs/dashboards/
#Mac: https://github.com/quarto-dev/quarto-cli/releases/download/v1.5.4/quarto-1.5.4-macos.pkg
```

```{r}
#Read and combine data files
files = Sys.glob('data/batch 1/*.csv')
data = files %>%
  map(~ read_csv(., show_col_types = F)) %>% 
  reduce(bind_rows)


#Preprocess
data = data %>% 
  group_by(SubjID) %>% 
  #pull out demographic responses into columns
  mutate(age = pick(everything()) %>% 
           filter(part_of_expt == 'demographics') %>% 
           pull(`demo_2--response`),
         gender = pick(everything()) %>% 
           filter(part_of_expt == 'demographics') %>% 
           pull(`demo_1--response`),
         race = pick(everything()) %>% 
           filter(part_of_expt == 'demographics') %>% 
           pull(`demo_0--response`),
         polparty = pick(everything()) %>% 
           filter(part_of_expt == 'party_choice') %>% 
           pull(responses) %>% 
           enter_object('Q0') %>% 
           pull(`..JSON`) %>% 
           pluck(1),
         polid_before = pick(everything()) %>% 
           filter(part_of_expt == 'party_confidence_before') %>% 
           pull(response),
         polid_after = pick(everything()) %>% 
           filter(part_of_expt == 'party_confidence_after') %>% 
           pull(response),
         groupid = pick(everything()) %>% 
           filter(part_of_expt == 'party_group_confidence') %>% 
           pull(response),
         #did they pass ALL attention checks
         passedattn = all(
           #heart attack attention check
           pick(everything()) %>% 
             filter(part_of_expt == 'attentioncheck_heart') %>% 
             pull(`attention_check_fatal_correctness`) == 'correct', 
           #sorting attention check
           pick(everything()) %>% 
             filter(part_of_expt == 'attentioncheck_sort') %>% 
             pull(`attention_check_sort_correctness`) == 'correct'),
         #What was the participant's last opinion
         participant_lastopinion = pick(everything()) %>% 
           filter(opinion_round == max(opinion_round, na.rm = T)) %>% 
           pull(participant_response),
         #What was the prediction about the new agent's opinion?
         prediction = pick(everything()) %>% 
           filter(part_of_expt == 'prediction') %>% 
           pull(agreement_prediction)) %>% 
  #correct for programming error (only use for V3/4/5/6 and Self 1)
  mutate(prediction = ifelse(prediction == 'agree',  'disagree', 'agree')) %>% 
  #Did new agent prediction match participant opinion on last round?
  mutate(pred_par = participant_lastopinion == prediction)
```

# Checks

## Demographics {.tabset title="Demographics (Attention Check)"}

```{r}
#| title: Passed
data %>% 
  filter(passedattn == T) %>% 
  select(SubjID, age, race, gender, polparty, Deviant_threshold) %>% 
  distinct() %>% 
  table1(~age+ race+ gender + polparty | Deviant_threshold, .)
```

```{r}
#| title: Failed
data %>% 
  filter(passedattn == F) %>% 
  select(SubjID, age, race, gender, polparty, Deviant_threshold) %>% 
  distinct() %>% 
  table1(~age+ race+ gender + polparty | Deviant_threshold, .)
```

```{r}
#Keep only those who passed attention checks
data = data %>% 
  filter(passedattn == T)
```

## Learning Analysis 

### Plot {.tabset title="Agent Learning Plots"}

```{r}
#| title: All Agents

#Process the learning data
learningtask = data %>%
  filter(part_of_expt == 'learning_task' &
           !grepl('participant', opinion_round)) %>% 
  mutate(corrresp = ifelse(response_correctness == 'correct', 1, 0))

# Calculate averages
learningavgs = learningtask %>% 
  # First find averages by participants
  group_by(SubjID, Deviant_threshold, opinion_round) %>% 
  summarize(subaccuracy = mean(corrresp) *100) %>% 
  # Then average again across participants
  group_by(Deviant_threshold, opinion_round) %>% 
  summarize(accuracy = mean(subaccuracy),
            n = n(),
            ci = qt(0.975,df=n-1)*sd(subaccuracy)/sqrt(n),
            l.ci= accuracy-ci,
            h.ci = accuracy+ci) %>% 
  # Change deviance to factor for coloring
  mutate(Deviant_threshold = as.factor(Deviant_threshold))

#Plot
(learningavgs %>%
  ggplot(aes(x=opinion_round,
             y=accuracy,
             color = Deviant_threshold,
             group = Deviant_threshold)) +
  geom_ribbon(aes(y = accuracy,
                  ymax = h.ci,
                  ymin = l.ci),
              alpha = .2) +
  geom_point() +
  geom_line() +
  theme_bw() +
  xlab('Round') +
  ylab("Accuracy")+
  facet_grid(.~ Deviant_threshold) +
  scale_y_continuous(labels = seq(0,100,10),
                     breaks = seq(0,100,10),
                     limits = c(0,101)) +
  scale_color_discrete(name = 'Deviation')) %>% 
  ggplotly()
```

### Model {.tabset title="Learning Analysis"}

```{r}
#| title: Model

learningmod = learningtask %>%
  mutate(corrresp = as.factor(corrresp),
         Deviant_threshold = factor(Deviant_threshold,
                                    levels= seq(0,1,.25)),
         opinion_round = as.numeric(opinion_round)) %>%
  glmer(corrresp ~ opinion_round * Deviant_threshold +
        (opinion_round | SubjID) +
        (1 | stim_person_name),
        family = binomial(link='logit'),
           control= glmerControl(optimizer="bobyqa",
                                optCtrl=list(maxfun=500000)),
      data = .)

#Overall significance
car::Anova(learningmod)

```

```{r}
#| title: Round

#Main effect of opinion round
summary(emtrends(learningmod, ~ 1, var="opinion_round"), infer= c(T,T))
```

```{r}
#| title: Deviance

#Main effect of deviance
summary(emmeans(learningmod, pairwise ~ Deviant_threshold, infer= c(T,T)))
```

# Clustering

## Similarity analysis 

### Plots {.tabset title="Similarity Analysis"}

```{r}
#| title: Plot

#Code the pairs
similarity = data %>% 
  filter(part_of_expt == 'similarity_rating') %>% 
  mutate(devpair = str_remove(stim_deviance_num_gend, '[ M]') %>% 
           str_remove('[ F]') %>% 
           str_remove('[1-4]') %>% 
           str_remove('[M]') %>% 
           str_remove('[ M]') %>% 
           str_remove('[F]') %>% 
           str_remove('[1-4]')) %>% 
  mutate(targetpair = ifelse(devpair == 'participant,maj_deviant' | devpair == 'maj_deviant,participant', 'PS', 'SS')) 

similarity_sub = similarity %>% 
  group_by(SubjID, Deviant_threshold, targetpair) %>% 
  summarize(subavg = mean(response))

similarity_avgs = similarity_sub %>% 
  group_by(Deviant_threshold, targetpair) %>% 
  summarize(avg = mean(subavg),
            n = n(),
            ci = qt(0.975,df=n-1)*sd(subavg)/sqrt(n),
            l.ci = avg-ci,
            h.ci = avg+ci)

(similarity_sub %>% 
    ggplot(aes(x = targetpair,
               y = subavg))+
    geom_violin()+
    geom_point(alpha = .2)+
    facet_grid(.~Deviant_threshold)+
    scale_y_continuous(name= 'Average Similarity',
                       breaks = seq(0,100,10))+
    theme_bw()+
    xlab('')+
    geom_point(data=similarity_avgs,
               aes(y=avg,
                   color=targetpair),
               size = 3,
               position = position_nudge(x=.25))+
    geom_errorbar(data=similarity_avgs,
                  aes(y=avg,
                      ymax=h.ci,
                      ymin=l.ci,
                      color=targetpair),
                  width=.2,
                  position = position_nudge(x=.25))+
  guides(fill = 'none', color='none')) %>% 
  ggplotly()
```

```{r}
#| title: Plot x PolParty

similarity_sub = similarity %>% 
  group_by(SubjID, Deviant_threshold, polparty, targetpair) %>% 
  summarize(subavg = mean(response))

similarity_avgs = similarity_sub %>% 
  group_by(Deviant_threshold, polparty, targetpair) %>% 
  summarize(avg = mean(subavg),
            n = n(),
            ci = qt(0.975,df=n-1)*sd(subavg)/sqrt(n),
            l.ci = avg-ci,
            h.ci = avg+ci)

(similarity_sub %>% 
    ggplot(aes(x = targetpair,
               y = subavg))+
    geom_violin()+
    geom_point(alpha = .2)+
    facet_grid(polparty~Deviant_threshold)+
    scale_y_continuous(name= 'Average Similarity',
                       breaks = seq(0,100,10))+
    theme_bw()+
    xlab('')+
    geom_point(data=similarity_avgs,
               aes(y=avg,
                   color=targetpair),
               size = 3,
               position = position_nudge(x=.25))+
    geom_errorbar(data=similarity_avgs,
                  aes(y=avg,
                      ymax=h.ci,
                      ymin=l.ci,
                      color=targetpair),
                  width=.2,
                  position = position_nudge(x=.25))+
  guides(fill = 'none', color='none')) %>% 
  ggplotly()
```

### Model {.tabset title="Similarity Analysis"}

```{r}
#| title: Model

similaritymod = similarity %>% 
  lmer(response ~ targetpair*Deviant_threshold +
         (targetpair|SubjID),
       REML = F,
           control= lmerControl(optimizer="bobyqa",
                                optCtrl=list(maxfun=500000)),
       data = .)

anova(similaritymod)
```

```{r}
#| title: Interxn

summary(emtrends(similaritymod, pairwise ~ targetpair, var = 'Deviant_threshold', infer= c(T,T)))
```

## ISM analysis

### Plot

```{r}
#| eval: false 

#Run the ISM using Julia
library(JuliaCall)
julia_setup(JULIA_HOME = "/Applications/Julia-1.8.app/Contents/Resources/julia/bin/")
julia_source(file_name = '../clustering/InfiniteSimilarityModel.jl')

runISM = function(X){
  julia_assign('X', X)
  julia_assign('chain', julia_eval('sample(ISM(X), SMC(), 5000);'))
  P = julia_eval('clusterprob(chain)')
  k = julia_eval('numclusters(chain)')
  list(P = P, k = k)
}
```

```{r}
#| eval: false 

#test if the function works as intended
library(Matrix)
clusterMatrix = function(k, obs){
  M = matrix(1, nrow = obs, ncol = obs)
  X = as.matrix(bdiag(rep(list(M), k)))
  X*.99
}
X = clusterMatrix(k=3, obs=8)

results = runISM(X)
results$P
mean(results$k)
```

```{r, eval = F}
#| eval: false 

#Create containers to record output
subs = unique(data[,c('SubjID', 'Deviant_threshold')])
subs$k = NA
submats = vector('list', length(subs$SubjID))
subprobmats = vector('list', length(subs$SubjID))
#make similarity matrix
for(i in 1:length(subs$SubjID)){
  subdat = similarity[similarity$SubjID == subs$SubjID[i],]
  #need unique names of the stimuli used
  subdat$stim1 = sapply(subdat$stim_names, function(s) strsplit(s, ',')[[1]][1])
  subdat$stim2 = sapply(subdat$stim_names, function(s) strsplit(s, ',')[[1]][2])
  allstim = unique(c(subdat$stim1, subdat$stim2))
  #use them to create matrix size
  mat = matrix(nrow = length(allstim), ncol = length(allstim))
  #name rows and columns by stimuli names so we can fill in
  colnames(mat) = allstim
  row.names(mat) = allstim
  #fill in matrix!
  for(row in 1:nrow(mat)){
    for(col in 1:ncol(mat)){
      if(row == col){ #dont have similarity to self, so give perfect similarity
        mat[row, col] = 100
        next
      }
      rname = row.names(mat)[row]
      cname = colnames(mat)[col]
      #find similarity response for pair
      mat[row, col] = subdat$response[subdat$stim_names == paste0(rname, ',', cname) | subdat$stim_names == paste0(cname, ',', rname)]
    }
  }
  subdat$dev1 = sapply(subdat$stim_deviance_num_gend, function(s) strsplit(s, ',')[[1]][1])
  subdat$dev2 = sapply(subdat$stim_deviance_num_gend, function(s) strsplit(s, ',')[[1]][2])
  devdf = unique(data.frame(stim = c(subdat$stim1, subdat$stim2), dev = c(subdat$dev1, subdat$dev2)))
  devdf$dev = gsub('nondeviant ', '', devdf$dev)
  for(b in 1:nrow(mat)){
    colnames(mat)[b] = devdf$dev[devdf$stim == colnames(mat)[b]]
    row.names(mat)[b] = devdf$dev[devdf$stim == row.names(mat)[b]]
  }
  #order so deviant always in upper left corner
  mat = mat[order(colnames(mat), decreasing = F), order(colnames(mat), decreasing = F)]
  
  #normalize for Turing function 
  mat = mat/100
  
  #save similarity matrix just in case
  submats[[i]] = mat
  
  #cluster model
  results = runISM(mat)
  
  #Save number of clusters
  subs$k[i] = mean(results$k)
  
  #estimate probability matrix between agents
  P = results$P
  colnames(P) = colnames(mat)
  row.names(P) = row.names(mat)
  
  #save probability matrix
  subprobmats[[i]] = P
  svMisc::progress(i, length(subs$SubjID))
  
  # par(mfrow=c(2,2))
  # corrplot::corrplot(mat)
  # corrplot::corrplot(P)
  # corrplot::corrplot(mat, order = 'hclust')
  # corrplot::corrplot(P, order = 'hclust')
}

#save the probability matrices & data frame so only have to run once (until something needs changing)
saveRDS(submats, file = "subsimmats.rds")
saveRDS(subprobmats, file = "subprobmats.rds")
saveRDS(subs, file = "subs-kestimates.rds")
```


```{r}
#| title: ISM Plot

#Read in data made from above code chunks
subprobmats = readRDS('subprobmats.rds')
subs = readRDS('subs-kestimates.rds')

kplot = subs %>% 
  group_by(Deviant_threshold) %>% 
  summarize(n = n(),
            avg = mean(k),
            ci = qt(.975, n-1)*sd(k)/sqrt(n),
            low.ci = avg - ci,
            hi.ci = avg + ci)

(subs %>% 
ggplot(aes(x = as.factor(Deviant_threshold),
           y = k,
           group = Deviant_threshold))+ 
  geom_violin(draw_quantiles = c(0.25, 0.75),
              linetype = "dashed") +
  geom_violin(fill="transparent",
              draw_quantiles = 0.5) +
  geom_point(alpha = .2)+
  theme_bw()+
  scale_y_continuous(name = 'k (number of clusters)')+
  geom_point(data=kplot,
             aes(y = avg),
             color = 'red')+
  geom_errorbar(data=kplot,
                aes(y = avg,
                    ymax = hi.ci,
                    ymin = low.ci),
                color = 'red',
                width = .2)+
  scale_x_discrete(name = 'Deviance')) %>% 
  ggplotly()
```

### Model {.tabset title="ISM Analysis"}

```{r}
#| title: Model

ismmod = subs %>% 
  mutate(Deviant_threshold = factor(Deviant_threshold,
                                   levels = seq(0,1,.25))) %>% 
  lm(k ~ Deviant_threshold, data = .)

anova(ismmod)
```

```{r}
#| title: Deviance

summary(emmeans(ismmod, pairwise ~ Deviant_threshold), infer = c(T,T))
```

```{r}
#| title: VS 2

emmeans(ismmod, ~ Deviant_threshold) %>% 
  test(., null = 2, side='<')
```

# Predictions

## Prediction Analysis

### Plot

```{r}
#| title: New Agent Prediction Plot

#Pull out confidence rating into column
predictions = data %>% 
  group_by(SubjID) %>% 
  mutate(confidence = pick(everything()) %>% 
           filter(part_of_expt == 'prediction_confidence') %>% 
           pull(response)) %>% 
  filter(part_of_expt == 'prediction')

#Compute Averages
predictions_avg = predictions %>% 
  group_by(Deviant_threshold) %>% 
  summarize(n = n(),
            m.conf = mean(confidence),
            ci = qt(0.975,df=n-1)*sd(confidence)/sqrt(n),
            l.ci= m.conf-ci,
            h.ci = m.conf+ci)

#Plot
(predictions %>% 
    ggplot(aes(x = as.factor(Deviant_threshold),
               y = confidence,
               group = Deviant_threshold))+ 
    geom_violin()+
    geom_point(alpha = .2)+
    geom_point(data = predictions_avg,
               aes(y = m.conf),
               color = 'red',
               size = 2)+
    geom_errorbar(data = predictions_avg,
                  aes(y = m.conf,
                      ymax=h.ci,
                      ymin=l.ci),
                  color = 'red',
                  width = .1)+
    theme_bw()+
    scale_y_continuous(labels = seq(0,100,10),
                       breaks = seq(0,100,10),
                       limits = c(0,100),
                       name = 'Confidence')+
    scale_x_discrete(labels = seq(0,1,.25),
                       breaks = seq(0,1,.25),
                       name = 'Deviance')) %>% 
  ggplotly()
```

### Model {.tabset title="Prediction Analysis"}

```{r}
#| title: Model

predictionmod = predictions %>% 
  mutate(deviance = factor(Deviant_threshold,
                                   levels = seq(0,1,.25))) %>% 
  lm(confidence ~ deviance, .)

anova(predictionmod)
```

```{r}
#| title: Deviance

summary(emmeans(predictionmod, pairwise ~ deviance), infer=c(T,T))
```

## Moderator Analysis 

### Moderator Last Opinion {.tabset title="Moderator: Last Opinion"}

```{r}
#| title: Table

table1(~pred_par | Deviant_threshold, predictions)
```

```{r}
#| title: Plot

predictions_par_avg = predictions %>% 
  group_by(Deviant_threshold, pred_par) %>% 
  summarize(n = n(),
            m.conf = mean(confidence),
            ci = qt(0.975,df=n-1)*sd(confidence)/sqrt(n),
            l.ci= m.conf-ci,
            h.ci = m.conf+ci)

(predictions %>% 
    ggplot(aes(x = as.factor(Deviant_threshold),
               y = confidence,
               group = Deviant_threshold))+ 
    geom_violin()+
    geom_point(data = predictions_par_avg,
               aes(y = m.conf),
               color = 'red',
               size = 2)+
    geom_errorbar(data = predictions_par_avg,
                  aes(y = m.conf,
                      ymax=h.ci,
                      ymin=l.ci),
                  color = 'red',
                  width = .1)+
    geom_point(alpha = .2)+
    theme_bw()+
    xlab('Deviance')+
    ylab("Confidence")+
  scale_y_continuous(labels = seq(0,100,10),
                     breaks = seq(0,100,10),
                     limits = c(0,100))+
    scale_x_discrete(labels = seq(0,1,.25),
                     breaks = seq(0,1,.25))+
    facet_grid(~pred_par)+
    ggtitle("Chose own opinion?")) %>% 
  ggplotly()
```

```{r}
#| title: Model

predictions %>% 
  mutate(deviance = as.factor(Deviant_threshold)) %>% 
  lm(confidence ~ deviance*pred_par, .) %>% 
  anova()
```

# Identification

## Stimulus Identification {.tabset title="ID with Stimulus Group"}

```{r}
#| title: Plot

stimid = data %>% 
  select(SubjID, Deviant_threshold, groupid) %>% 
  distinct()

stimid_avg = stimid %>% 
  group_by(Deviant_threshold) %>% 
  summarize(n = n(),
            m.conf = mean(groupid),
            ci = qt(0.975,df=n-1)*sd(groupid)/sqrt(n),
            l.ci= m.conf-ci,
            h.ci = m.conf+ci)
(stimid %>% 
    ggplot(aes(x = as.factor(Deviant_threshold),
               y = groupid,
               group = Deviant_threshold))+ 
    geom_violin()+
    geom_point(data = stimid_avg,
               aes(y = m.conf),
               color = 'red',
               size = 2)+
    geom_errorbar(data = stimid_avg,
                  aes(y = m.conf,
                      ymax=h.ci,
                      ymin=l.ci),
                  color = 'red',
                  width = .1)+
    geom_point(alpha = .2)+
    theme_bw()+
    xlab('Deviance')+
    ylab("Identification")+
  scale_y_continuous(labels = seq(0,100,10),
                     breaks = seq(0,100,10),
                     limits = c(0,100))+
    scale_x_discrete(labels = seq(0,1,.25),
                     breaks = seq(0,1,.25))) %>% 
  ggplotly()

```

```{r}
#| title: Model

stimidmod = stimid %>% 
  lm(groupid ~ Deviant_threshold, .) 

stimidmod %>% 
  anova()
```

```{r}
#| title: Deviance

summary(emtrends(stimidmod, ~ 1, var = "Deviant_threshold"), infer=c(T,T))
```

## Political Identification {.tabset title="ID with PolParty"}

```{r}
#| title: Plot

polid = data %>% 
  select(SubjID, Deviant_threshold, polid_before, polid_after) %>% 
  distinct() %>% 
  pivot_longer(
    cols = starts_with('polid'),
    names_to = 'time',
    values_to = 'id',
    names_prefix = 'polid_'
  ) %>% 
  mutate(time = factor(time,
                       levels = c('before', 'after')),
         Deviant_threshold = factor(Deviant_threshold,
                                    levels = seq(0,1,.25)))

polid_avg = polid %>% 
  group_by(Deviant_threshold, time) %>% 
  summarize(n = n(),
            m.conf = mean(id),
            ci = qt(0.975,df=n-1)*sd(id)/sqrt(n),
            l.ci= m.conf-ci,
            h.ci = m.conf+ci)

(polid %>% 
    ggplot(aes(x = time,
               y = id))+ 
    geom_violin(aes(fill = time))+
    geom_line(aes(group = SubjID),
              alpha = .2) +
    geom_point(data = polid_avg,
               aes(y = m.conf),
               #color = 'red',
               size = 2)+
    geom_errorbar(data = polid_avg,
                  aes(y = m.conf,
                      ymax=h.ci,
                      ymin=l.ci),
                  #color = 'red',
                  width = .1)+
    #geom_point(alpha = .2)+
    theme_bw()+
    xlab('Time')+
    ylab("Identification")+
  scale_y_continuous(labels = seq(0,100,10),
                     breaks = seq(0,100,10),
                     limits = c(0,100))+
    facet_grid(. ~ Deviant_threshold)+
    guides(fill = 'none')) %>% 
  ggplotly()

```

```{r}
#| title: Model

polmod = polid %>% 
  lmer(id ~ Deviant_threshold*time +
         (1|SubjID),
       REML = F,
           control= lmerControl(optimizer="bobyqa",
                                optCtrl=list(maxfun=500000)),
       data = .)

anova(polmod)
```


# Notes

::: {.card title="Things to note"}

- Similarity Analysis, doesn't make sense to compare "deviant" vs "nondeviant" learning in this design where deviance is spread across agents (there is no specific deviant).

- On the prediction of the new agent's choice based on the last opinion, the moderator here is whether participants chose the same respond they themselves did.

:::

::: {.card title="Unresolved"}

- Learning analysis, how correctness was coded in the learning phase (is button an issue?, show data examples)

- U shape analysis, affects all studies
:::

